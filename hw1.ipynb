{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning: Assignment 1\n",
    "\n",
    "This assignment is intended to help introduce you to programming with Python, and give you a first taste of applying simple Machine Learning algorithms from a toolkit. You will then write your own implementation of a simple machine learning algorithm, which you can compare to the toolkit version to check your work. Finally, you will answer some short written questions based on the output of your program.\n",
    "\n",
    "Students registered for 8515 must complete this assignment individually (i.e. without collaboration). Students registered for 4510 may optionally work in pairs. If working as a pair, make sure that all submitted files have both students names on them. Note that “working as a pair” means pair programming (google this term if you’re not familiar with it); it does *not* mean one student doing all the work while the other watches, or splitting up the work and each doing half of it, etc.. If working with a partner doesn’t result in both of you learning more/faster than you would on your own, you’re doing it wrong.  I recommend that students who are familiar with Python work by themselves, but if you're new to Python then finding another newbie to work with is a good idea.  Try not to work with someone of a drastically different skill level than you, since it's actually a lot harder to learn effectively that way.\n",
    "\n",
    "The three parts of this assignment (parts A, B, and C, all in this file) should be submitted on Blackboard.  Parts A and B are due at 11pm next Wednesday (i.e. the night before class in one week). Part C will be due the following week.  Be sure to start early so you have time to complete all of them. If you get stuck, don’t hesitate to come to office hours or ask for help on Piazza."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Python Refresher\n",
    "This first part of the assignment is intended to be a quick introduction to Jupyter Notebooks, and also a refresher on basic Python syntax.  If you're less experienced with Python, you may want to start by working through some tutorials online (e.g. https://docs.python.org/3.7/tutorial/).\n",
    "\n",
    "Before you start, be sure to re-name this file to \"hw1.ipynb\"; don't leave the \"scaffolding\" in the filename of the version you submit.  You should also move the file to some appropriate directory, like maybe \"MachineLearning/Code\" in your Documents folder.  You should perform these steps for all future assignments with scaffolding as well, even if it's not explicitly stated in the directions of later assignments.\n",
    "\n",
    "The first thing you should do is modify the comment block below, filling in your information as appropriate.  Once you've done that, read through the text and run the blocks of code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSC 4510 - Machine Learning\n",
    "# Assignment 1\n",
    "# Scaffolding by Dr. Ben Mitchell\n",
    "# Assignment completed by: <C.SAI SINDHU>\n",
    "# Resources used: \n",
    "#   <List any resources you used beyond the ones posted on Blackboard>\n",
    "#   <This can include books, websites, other students, etc.>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Jupyter Notebook is organized into blocks called 'cells.'  Each cell can contain either code or markdown.  You can 'run' the cell by clicking the little forward-arrow symbol in the toolbar at the top of the page, or by pressing [Shift-Enter].  For a code cell, this will execute the Python code in that cell, and then display any result that code produces below.  For a markdown cell (like this one), it will transform the markdown text into pretty, textbook-style text.  If you double click on a markdown cell, you can edit the raw text, and then 'run' it to re-compile the pretty version.  For more about the things you can do with markdown, check out this 'cheet-sheet' for markdown commands: https://medium.com/ibm-data-science-experience/markdown-for-jupyter-notebooks-cheatsheet-386c05aeebed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below contains some Python code.  Because Python is an interpreted language, you can run individual statements and get results.  If you run the following cell, you should get the answer '10'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3 + 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous cell ran a calculation, but it didn't store the result anywhere.  If we want to do anything beyond just displaying the result, we need to store it in a variable.  Python variables are auto-typed, meaning you don't need to declare them with a typename the way you do in languages like Java or C; if you assign an integer to a variable name, Python automatically creates an integer variable with that name.  Note that when you run the following cell, there's no output displayed, because the assignment doesn't have an output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "myNumber = 7 + 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Loops and conditionals in Python have a different syntax than in Java, but ultimately work similarly.  Note that we use a ':' to say we're starting the body of the statement, and then use indentation to indicate what code is 'inside' the loop or conditional.  When the indent goes back out, the loop is over.  Here's a simple example of a Python loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,3):\n",
    "    print(i)\n",
    "print(\"done\")\n",
    "\n",
    "# this is equivalent to the Java loop:\n",
    "# for (int i=0; i<3; i++) {\n",
    "#    System.out.println(i);\n",
    "# }\n",
    "# System.out.println(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python lists (which are like arrays in other languages) also work by auto-typed assignment; the following cell creates three different variables, each of which stores a list of numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1, 2, 3]\n",
    "b = [3, 7, -2, 5]\n",
    "c = [22, 3, 97, 12, 15, 72, 8, 34, 60]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python loops actually iterate over a list of items.  The 'range' statement above generates a list of numbers, but we can also hand a list directly to the loop.  For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "7\n",
      "-2\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "for x in b:\n",
    "    print(x)\n",
    "    \n",
    "# the following is an alternate way to accomplish the same thing:\n",
    "#for i in range(0,len(b)):\n",
    "#    print(b[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By defualt, Python does assignment by reference, so the following doesn't actually make a copy of all the values stored in *a*, instead it makes *lst* refer to the same array that *a* does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = b\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an example of using a loop and an if statement to find the maximum value in a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max is:  7\n"
     ]
    }
   ],
   "source": [
    "max = 0\n",
    "for x in lst:\n",
    "    if x > max:\n",
    "        max = x\n",
    "print('max is: ', max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try changing the cell two above to make 'lst' refer to 'b' instead of 'a', and then re-run the cell to calculate 'max' to see the difference.  Note that just changing the assignment doesn't actually do anything; you need to re-run the assignment cell in order for the modified code to actually do anything.\n",
    "\n",
    "It's also worth noting that when you save a Notebook, the cell outputs get saved, but you'll still need to re-run all the cells from the top of the file before things will work correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining functions/methods works similarly, using the colon and indentation with the keyword 'def'.  Here's an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMax(someList):\n",
    "    max = 0\n",
    "    for x in someList:\n",
    "        if x > max:\n",
    "            max = x\n",
    "    return max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that you need to run the cell with the function definition and then call the function before it will do anything:a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getMax(lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part A\n",
    "\n",
    "Fill in the next three cells to calculate the sum (i.e. total), the minimum, and the mean (i.e. average) for 'lst'.  Be sure to test your code using multiple lists (you can use the ones defined above by just making 'lst' refer to a, b, or c)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total is:  13\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "def getTotal(someList):\n",
    "    total = 0\n",
    "    for x in someList:\n",
    "        total+=x;\n",
    "    return total\n",
    "    \n",
    "print('total is: ', getTotal(b));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum is:  -2\n"
     ]
    }
   ],
   "source": [
    "def getMin(someList):\n",
    "    min = float('inf')\n",
    "    for x in someList:\n",
    "        if x < min:\n",
    "            min = x\n",
    "    return min\n",
    "\n",
    "print('Minimum is: ', getMin(b));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average is:  3.25\n"
     ]
    }
   ],
   "source": [
    "#Todo: write code to calculate and print the average value of the elements in the list\n",
    "\n",
    "def getAvg(someList):\n",
    "    avg=(getTotal(someList))/len(someList)\n",
    "    return avg\n",
    "\n",
    "print('Average is: ', getAvg(b));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part B\n",
    "\n",
    "Modify this markdown cell to add the answers to the following short-answer questions.  In doing so, consult the code below, which shows an example of using SciKit Learn to load and classify a simple dataset.  Note that you may need to make minor modifications to the example code for some answers (e.g. changing the number of neighbors used, running on the train vs test set)\n",
    "\n",
    "1. What is the full name of this data set?\n",
    "Iris plants dataset\n",
    "\n",
    "2. What sort of real-world user might be interested in a system that could successfully solve this classification problem?\n",
    "1.botanist who want to classify flowers\n",
    "\n",
    "2.students and people interested in data science,this may serve as starting example to work on.\n",
    "The data is real data, but apparently of good quality. In principle and in practice, test datasets could be synthetic and that might be necessary or useful to make a point. Nevertheless, few people object to real data.comparing methods old and new, and in evaluating any method, it is often considered helpful to try them out on known datasets, thus maintaining some continuity in how we assess methods.\n",
    "3. How many features (not including the class label) does each example in the data set have?\n",
    " 4\n",
    "4. How many examples does the data set contain?\n",
    " 150\n",
    "\n",
    "5. What are the available class labels?\n",
    " - Iris-Setosa\n",
    " - Iris-Versicolour\n",
    " - Iris-Virginica\n",
    "\n",
    "6. When run with a 60/40 train/test split, what is the accuracy of a Nearest Neighbor classifier on the test set?\n",
    "0.9166666666666666\n",
    "\n",
    "7. Accuracy of 3-nearest neighbor?\n",
    "0.9333333333333333\n",
    "8. Accuracy of 5-nearest neighbor?\n",
    "0.95\n",
    "9. Accuracy of 20-nearest neighbor?\n",
    "0.9166666666666666\n",
    "10. What is the accuracy of nearest neighbor on the training set (*not* the test set)?\n",
    "0.9777777777777777(using 20-nearest neighbour)\n",
    "11. Is there a difference between the accuracy on the training set and accuracy on the test set?  Explain why the observed behavior occurs.\n",
    "yes,the difference arised because we are feeding the testdata and again checking on the same,so it improves accuracy , but the training data set is completely different from test and the developed model has signficant deviations from test data set,so the accuracy might have reduced. if we increase the near-neighbour the accuracy is decreasing signifintly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries we'll need...\n",
    "from math import sqrt\n",
    "import numpy as np\n",
    "from sklearn import neighbors, datasets\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# then load some data...\n",
    "iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _iris_dataset:\n",
      "\n",
      "Iris plants dataset\n",
      "--------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    :Number of Instances: 150 (50 in each of three classes)\n",
      "    :Number of Attributes: 4 numeric, predictive attributes and the class\n",
      "    :Attribute Information:\n",
      "        - sepal length in cm\n",
      "        - sepal width in cm\n",
      "        - petal length in cm\n",
      "        - petal width in cm\n",
      "        - class:\n",
      "                - Iris-Setosa\n",
      "                - Iris-Versicolour\n",
      "                - Iris-Virginica\n",
      "                \n",
      "    :Summary Statistics:\n",
      "\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "                    Min  Max   Mean    SD   Class Correlation\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "    sepal length:   4.3  7.9   5.84   0.83    0.7826\n",
      "    sepal width:    2.0  4.4   3.05   0.43   -0.4194\n",
      "    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\n",
      "    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "    :Class Distribution: 33.3% for each of 3 classes.\n",
      "    :Creator: R.A. Fisher\n",
      "    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n",
      "    :Date: July, 1988\n",
      "\n",
      "The famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\n",
      "from Fisher's paper. Note that it's the same as in R, but not as in the UCI\n",
      "Machine Learning Repository, which has two wrong data points.\n",
      "\n",
      "This is perhaps the best known database to be found in the\n",
      "pattern recognition literature.  Fisher's paper is a classic in the field and\n",
      "is referenced frequently to this day.  (See Duda & Hart, for example.)  The\n",
      "data set contains 3 classes of 50 instances each, where each class refers to a\n",
      "type of iris plant.  One class is linearly separable from the other 2; the\n",
      "latter are NOT linearly separable from each other.\n",
      "\n",
      ".. topic:: References\n",
      "\n",
      "   - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\n",
      "     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\n",
      "     Mathematical Statistics\" (John Wiley, NY, 1950).\n",
      "   - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\n",
      "     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\n",
      "   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\n",
      "     Structure and Classification Rule for Recognition in Partially Exposed\n",
      "     Environments\".  IEEE Transactions on Pattern Analysis and Machine\n",
      "     Intelligence, Vol. PAMI-2, No. 1, 67-71.\n",
      "   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\n",
      "     on Information Theory, May 1972, 431-433.\n",
      "   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\n",
      "     conceptual clustering system finds 3 classes in the data.\n",
      "   - Many, many more ...\n"
     ]
    }
   ],
   "source": [
    "# let's check out the description:\n",
    "print(iris.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 4)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's see what it looks like\n",
    "iris.data.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150,)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.1, 3.5, 1.4, 0.2],\n",
       "       [4.9, 3. , 1.4, 0.2],\n",
       "       [4.7, 3.2, 1.3, 0.2],\n",
       "       [4.6, 3.1, 1.5, 0.2],\n",
       "       [5. , 3.6, 1.4, 0.2],\n",
       "       [5.4, 3.9, 1.7, 0.4],\n",
       "       [4.6, 3.4, 1.4, 0.3],\n",
       "       [5. , 3.4, 1.5, 0.2],\n",
       "       [4.4, 2.9, 1.4, 0.2],\n",
       "       [4.9, 3.1, 1.5, 0.1],\n",
       "       [5.4, 3.7, 1.5, 0.2],\n",
       "       [4.8, 3.4, 1.6, 0.2],\n",
       "       [4.8, 3. , 1.4, 0.1],\n",
       "       [4.3, 3. , 1.1, 0.1],\n",
       "       [5.8, 4. , 1.2, 0.2],\n",
       "       [5.7, 4.4, 1.5, 0.4],\n",
       "       [5.4, 3.9, 1.3, 0.4],\n",
       "       [5.1, 3.5, 1.4, 0.3],\n",
       "       [5.7, 3.8, 1.7, 0.3],\n",
       "       [5.1, 3.8, 1.5, 0.3],\n",
       "       [5.4, 3.4, 1.7, 0.2],\n",
       "       [5.1, 3.7, 1.5, 0.4],\n",
       "       [4.6, 3.6, 1. , 0.2],\n",
       "       [5.1, 3.3, 1.7, 0.5],\n",
       "       [4.8, 3.4, 1.9, 0.2],\n",
       "       [5. , 3. , 1.6, 0.2],\n",
       "       [5. , 3.4, 1.6, 0.4],\n",
       "       [5.2, 3.5, 1.5, 0.2],\n",
       "       [5.2, 3.4, 1.4, 0.2],\n",
       "       [4.7, 3.2, 1.6, 0.2],\n",
       "       [4.8, 3.1, 1.6, 0.2],\n",
       "       [5.4, 3.4, 1.5, 0.4],\n",
       "       [5.2, 4.1, 1.5, 0.1],\n",
       "       [5.5, 4.2, 1.4, 0.2],\n",
       "       [4.9, 3.1, 1.5, 0.2],\n",
       "       [5. , 3.2, 1.2, 0.2],\n",
       "       [5.5, 3.5, 1.3, 0.2],\n",
       "       [4.9, 3.6, 1.4, 0.1],\n",
       "       [4.4, 3. , 1.3, 0.2],\n",
       "       [5.1, 3.4, 1.5, 0.2],\n",
       "       [5. , 3.5, 1.3, 0.3],\n",
       "       [4.5, 2.3, 1.3, 0.3],\n",
       "       [4.4, 3.2, 1.3, 0.2],\n",
       "       [5. , 3.5, 1.6, 0.6],\n",
       "       [5.1, 3.8, 1.9, 0.4],\n",
       "       [4.8, 3. , 1.4, 0.3],\n",
       "       [5.1, 3.8, 1.6, 0.2],\n",
       "       [4.6, 3.2, 1.4, 0.2],\n",
       "       [5.3, 3.7, 1.5, 0.2],\n",
       "       [5. , 3.3, 1.4, 0.2],\n",
       "       [7. , 3.2, 4.7, 1.4],\n",
       "       [6.4, 3.2, 4.5, 1.5],\n",
       "       [6.9, 3.1, 4.9, 1.5],\n",
       "       [5.5, 2.3, 4. , 1.3],\n",
       "       [6.5, 2.8, 4.6, 1.5],\n",
       "       [5.7, 2.8, 4.5, 1.3],\n",
       "       [6.3, 3.3, 4.7, 1.6],\n",
       "       [4.9, 2.4, 3.3, 1. ],\n",
       "       [6.6, 2.9, 4.6, 1.3],\n",
       "       [5.2, 2.7, 3.9, 1.4],\n",
       "       [5. , 2. , 3.5, 1. ],\n",
       "       [5.9, 3. , 4.2, 1.5],\n",
       "       [6. , 2.2, 4. , 1. ],\n",
       "       [6.1, 2.9, 4.7, 1.4],\n",
       "       [5.6, 2.9, 3.6, 1.3],\n",
       "       [6.7, 3.1, 4.4, 1.4],\n",
       "       [5.6, 3. , 4.5, 1.5],\n",
       "       [5.8, 2.7, 4.1, 1. ],\n",
       "       [6.2, 2.2, 4.5, 1.5],\n",
       "       [5.6, 2.5, 3.9, 1.1],\n",
       "       [5.9, 3.2, 4.8, 1.8],\n",
       "       [6.1, 2.8, 4. , 1.3],\n",
       "       [6.3, 2.5, 4.9, 1.5],\n",
       "       [6.1, 2.8, 4.7, 1.2],\n",
       "       [6.4, 2.9, 4.3, 1.3],\n",
       "       [6.6, 3. , 4.4, 1.4],\n",
       "       [6.8, 2.8, 4.8, 1.4],\n",
       "       [6.7, 3. , 5. , 1.7],\n",
       "       [6. , 2.9, 4.5, 1.5],\n",
       "       [5.7, 2.6, 3.5, 1. ],\n",
       "       [5.5, 2.4, 3.8, 1.1],\n",
       "       [5.5, 2.4, 3.7, 1. ],\n",
       "       [5.8, 2.7, 3.9, 1.2],\n",
       "       [6. , 2.7, 5.1, 1.6],\n",
       "       [5.4, 3. , 4.5, 1.5],\n",
       "       [6. , 3.4, 4.5, 1.6],\n",
       "       [6.7, 3.1, 4.7, 1.5],\n",
       "       [6.3, 2.3, 4.4, 1.3],\n",
       "       [5.6, 3. , 4.1, 1.3],\n",
       "       [5.5, 2.5, 4. , 1.3],\n",
       "       [5.5, 2.6, 4.4, 1.2],\n",
       "       [6.1, 3. , 4.6, 1.4],\n",
       "       [5.8, 2.6, 4. , 1.2],\n",
       "       [5. , 2.3, 3.3, 1. ],\n",
       "       [5.6, 2.7, 4.2, 1.3],\n",
       "       [5.7, 3. , 4.2, 1.2],\n",
       "       [5.7, 2.9, 4.2, 1.3],\n",
       "       [6.2, 2.9, 4.3, 1.3],\n",
       "       [5.1, 2.5, 3. , 1.1],\n",
       "       [5.7, 2.8, 4.1, 1.3],\n",
       "       [6.3, 3.3, 6. , 2.5],\n",
       "       [5.8, 2.7, 5.1, 1.9],\n",
       "       [7.1, 3. , 5.9, 2.1],\n",
       "       [6.3, 2.9, 5.6, 1.8],\n",
       "       [6.5, 3. , 5.8, 2.2],\n",
       "       [7.6, 3. , 6.6, 2.1],\n",
       "       [4.9, 2.5, 4.5, 1.7],\n",
       "       [7.3, 2.9, 6.3, 1.8],\n",
       "       [6.7, 2.5, 5.8, 1.8],\n",
       "       [7.2, 3.6, 6.1, 2.5],\n",
       "       [6.5, 3.2, 5.1, 2. ],\n",
       "       [6.4, 2.7, 5.3, 1.9],\n",
       "       [6.8, 3. , 5.5, 2.1],\n",
       "       [5.7, 2.5, 5. , 2. ],\n",
       "       [5.8, 2.8, 5.1, 2.4],\n",
       "       [6.4, 3.2, 5.3, 2.3],\n",
       "       [6.5, 3. , 5.5, 1.8],\n",
       "       [7.7, 3.8, 6.7, 2.2],\n",
       "       [7.7, 2.6, 6.9, 2.3],\n",
       "       [6. , 2.2, 5. , 1.5],\n",
       "       [6.9, 3.2, 5.7, 2.3],\n",
       "       [5.6, 2.8, 4.9, 2. ],\n",
       "       [7.7, 2.8, 6.7, 2. ],\n",
       "       [6.3, 2.7, 4.9, 1.8],\n",
       "       [6.7, 3.3, 5.7, 2.1],\n",
       "       [7.2, 3.2, 6. , 1.8],\n",
       "       [6.2, 2.8, 4.8, 1.8],\n",
       "       [6.1, 3. , 4.9, 1.8],\n",
       "       [6.4, 2.8, 5.6, 2.1],\n",
       "       [7.2, 3. , 5.8, 1.6],\n",
       "       [7.4, 2.8, 6.1, 1.9],\n",
       "       [7.9, 3.8, 6.4, 2. ],\n",
       "       [6.4, 2.8, 5.6, 2.2],\n",
       "       [6.3, 2.8, 5.1, 1.5],\n",
       "       [6.1, 2.6, 5.6, 1.4],\n",
       "       [7.7, 3. , 6.1, 2.3],\n",
       "       [6.3, 3.4, 5.6, 2.4],\n",
       "       [6.4, 3.1, 5.5, 1.8],\n",
       "       [6. , 3. , 4.8, 1.8],\n",
       "       [6.9, 3.1, 5.4, 2.1],\n",
       "       [6.7, 3.1, 5.6, 2.4],\n",
       "       [6.9, 3.1, 5.1, 2.3],\n",
       "       [5.8, 2.7, 5.1, 1.9],\n",
       "       [6.8, 3.2, 5.9, 2.3],\n",
       "       [6.7, 3.3, 5.7, 2.5],\n",
       "       [6.7, 3. , 5.2, 2.3],\n",
       "       [6.3, 2.5, 5. , 1.9],\n",
       "       [6.5, 3. , 5.2, 2. ],\n",
       "       [6.2, 3.4, 5.4, 2.3],\n",
       "       [5.9, 3. , 5.1, 1.8]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='euclidean',\n",
       "           metric_params=None, n_jobs=None, n_neighbors=20, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's train a nearest neighbor classifier...\n",
    "# note that we can use 'n_neighbors' to control the \"K\" in our KNN\n",
    "classifierA = neighbors.KNeighborsClassifier(metric='euclidean', n_neighbors=20)\n",
    "classifierA.fit(iris.data, iris.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.98"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ...then test it out to see how it performs (the result is the percentage of examples it gets right)\n",
    "classifierA.score(iris.data, iris.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training shape: (90, 4) , testing shape: (60, 4)\n"
     ]
    }
   ],
   "source": [
    "# ...hmm, that seems too good to be true; probably because we used the same data for training and testing!\n",
    "# lets try splitting our data into two parts so we can get a more realistic idea of how this would work\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.4, random_state=0)\n",
    "\n",
    "print(\"training shape:\", X_train.shape, \", testing shape:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9777777777777777"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now we can re-train our classifier using just the \"training\" part of the data:\n",
    "classifierB = neighbors.KNeighborsClassifier(metric='euclidean', n_neighbors=20)\n",
    "classifierB.fit(X_train, y_train)\n",
    "classifierB.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part C\n",
    "\n",
    "The final part of this assignment asks you to implement your own Nearest Neighbor classifier. __Note that this part is due a week later than the first two parts__, but you are still encouraged to start early.  For this part of the assignment, you’ve already got scaffolding (above) that shows how to use the Nearest Neighbor classifier from the toolkit. Your job is to write your own nearest neighbor classifier, which should be give the same answer as the one from Scikit Learn.\n",
    "\n",
    "- Start be defining a distance function, which should take two inputs (you can assume each will be a numpy array), and return the Euclidean distance between them.\n",
    "\n",
    "- Test this function thoroughly to be sure it works as intended.\n",
    "\n",
    "- Next, define a function to take 4 inputs: training data, training labels, testing data, and testing labels (note that the prototype has been written for you). Fill in the body of this function so that it loops over the examples in the test set, and for each one performs a nearest-neighbor classification. The function should print out the overall accuracy (i.e. the number of times the predicted label matched the true label for the testing examples, divided by the total number of testing examples). This function should make use of the distance function you wrote in the previous step.\n",
    "\n",
    "- Note that you may create additional helper functions if you wish\n",
    "\n",
    "- Test this function on the same train/test data you used with the SciKit Learn nearest neighbor classifier; your function should produce the same accuracy as the version from the toolkit (at least to the first three or four significant figures).\n",
    "\n",
    "- __CSC 8515 only:__ students enrolled in the graduate section of the course are also required to extend this function to take a number of neighbors as a parameter. Again, you should be able to try it with different numbers of neighbors, and get the same results as you would with SciKit Learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: implement this function\n",
    "# note that the lack of types in Python means argument types can be confusing;\n",
    "# in this case, 'a' and 'b' should be arrays, and the function should compute Euclidean distance between them,\n",
    "# which should be returned as a scalar\n",
    "def distance(a, b):\n",
    "    print(\"<not yet implemented>\")\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check this result by hand to make sure your distance function works; then check it using some other point pairs to be sure\n",
    "distance(X_train[0], X_train[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: implement this function\n",
    "def myNearestNeighbor(train, trainLabels, test, testLabels):\n",
    "    print(\"accuracy: <not yet implemented>\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myNearestNeighbor(X_train, y_train, X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
